\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}


\makeatletter


\providecommand{\tabularnewline}{\\}


\makeatother

\chapter{Modeling}

\section{Introduction}
%introduce actual fit, fit regions, systematics treatment, results of fit stages
The modeling for this analysis uses the standard counting experiment approach with a Poisson likelihood. The MC model is data driven such that background composes most of the regions and tunes the data and MC agreement. This agreement translates into well constrained background predictions in sensitive regions with sparse background with an ABCD-like approach. To achieve a robust fit model three stages of fits are performed, one with the Control Region (CR) which has no signal presence, a Validation Region (VR) which is a partially unblinded region with mild sensitivity designed to validate the CR model and demonstrate reasonable modeling in regions untouched by the CR , and finally SR which is comprised of the high $R_{ISR}$ bins which are sensitive to all signal regions. Following the full fit, limits can be calculated which test the hypotheses of background only model versus background plus signal model. 


\section{Fit Strategy and Fit Region Definitions}
%CR VR SR
The strategy for developing a robust fit model with the ability to accurately predict the background in the most sensistive regions is carried out in three stages. The stages combine different regions with varying sensitivity, they are the Control Region, Validation Region, and Signal Region. The control region guaranteed to have no signal sensitivity and also covers nearly a majority of all bins. Quantitatively, the expected contamination in the control region from any signal process being stops,sleptons, or electroweakinos is $<1\%$ in and below sparticle masses that are of potential discovery interest. Signals can appear in the CR, especially stops, due to the strong cross-section, but, the mass ranges that can appear at the few percent level or more have already been well excluded by other searches. The categories and bins that compose the CR are easily described as simply the low $R_{ISR}$ region and more specifically the lowest two $R_{ISR}$ bins of almost every category. The only category that has been specifically excluded from the CR is 2L high $p_T^{ISR}$ categories which were especially sensitive to some stop signals and borderline sensitive to electroweakinos. The CR region is then fully described by XYZ categories and ABC total bins, of which, hold $x\%$ of total expected Run II events. This means that the CR will ultimately dominate the behavior of the fit, but, it does not guarantee everywhere will be well modeled e.g. high $R_{ISR}$. To deal with this short coming, we introduce a complementary region the Validation Region. This region is designed to extend the control region to sample all types of categories and kinematics. The VR unmasks all the bronze categories, so it is composed the remaining bronze category $R_{ISR}$ not covered by the CR. Fitting the CR+VR is a conservative approach unblinding useful in validating the fit model everywhere. The signal presence in VR bins is mild where stops and electroweakinos can show up in the most sensitive categories, like $\mu\mu$, at the few $\%$ level. Together the CR+VR fit covers XYZ categories, and ABC total bins with an increase in the expected number of events to $y\%$. The remaining region is the signal region SR, it comprises the XYX categories and ABC total bins, and a smaller fraction of $z\%$ of total expected events. The sensitivity to every signal is high in this region examples of the S/B significance using the Zbinomial statistic if Fig X.

\FigTwo{Model_figs/0L_4J_ratioZbin_highDm_RISR_Mperp.pdf}{Model_figs/1L_G_0J_ratioZbin_chargeSep_RISR_Mperp.pdf}{stop in 0l and 1l}{fig:zbi1}

\FigTwo{Model_figs/zbi_2lgold.png}{Model_figs/zbi_2lbron.png}{all signals 2l}{fig:zbi2}




\section{Fit Implementation and Model Defintion}
%Poisson likelihood
The fitting framework is provided by the HiggsCombine tool which generates datacards that encodes all the components of the fit into a standard format that is processed by CombineHarvester and RooFit/RooStats packages. The fit and its components can be represented by a Poisson likelihood defined as:
\begin{equation}
\label{eq:fit}
\mathcal{L}(\vec{\alpha}|\vec{x}) = \bigg[ \prod_i^N \text{Pois}(x_i|\lambda_i(\vec{\alpha})) \bigg] \bigg[\prod_j^M \pi_j(\alpha_j) \bigg]
\end{equation}
The contents of equation \ref{eq:fit} extend over the range of all $N$ analysis bins where each $i$-th bin is composed of a count of observed events $x_i$ and expected events $\lambda_i$. The expected events are subject to the set of nuisance parameters $\vec{\alpha}$ of which some are conditioned by prior probability distributions $\pi_j(\alpha_j)$. The ideal model for $\lambda(\vec{\alpha})$ is found by maximizing the likelihood \ref{eq:fit} with the minimum set of nuisance parameters $\vec{\alpha}$ by fitting the stages of fit regions such that the model is sensitive to signals and the signal + background hyptothesis.  There are three types of nuisances implemented in the fit, freely floating rate parameters, log-normal constrained parameters, and shape parameters.  Freely floating paramters contribute to a factor $\kappa$, with a starting value of 1,  that is applied to the expected bin yield $\lambda$ to adjust the bin yield by some fraction with respect to the nominal value. The free parameters have no associated penalty with their adjustment and are fully determined by data. Individual bins $i$ are mapped together by common processes $k$ which are all associated under a common nuisance $j$. The selection of processes associated to a nuisance parameter can either be the contribution from a background process or associated fake leptons. The definition of a free rate parameter can then be defined as 
\begin{equation}
\label{eq:rateparam}
\kappa_{ijk}(\alpha_j) = \alpha_j
\end{equation}  
The log-normal parameters also functions of a $\kappa$ factor that is applied to an expected events of the associated bin. The log-normal parameter is different from the freely floating paramters in such that it is penalized for moving from the nominal value with based on normally distributed prior $\pi(\alpha_j)$. Along with a prior associated uncertainty on a process $j$ of nuisance $k$, that is $\sigma_{jk}$, the log-normal factors are defined as
\begin{equation}
\label{eq:logparam}
\kappa_{ijk}(\alpha_j) = (1+\sigma_{ijk})^{\alpha_j}
\end{equation}

The third type of nuisance is different from the first two such that it does not associate with and modify the process components for a particular bin, instead, the shape nuisances adjust expected bin yields based on the underlying shapes of the $R_{ISR}$ and $M_\perp$ distributions. The $kappa$ factor for a bin yield is then a function of up and down variations of one of the kinematic variables which are also encoded with a normally distributed prior $\pi(\alpha_j)$. The $\kappa$ definition is based on the interpolation $-1<\alpha_j<1$ and is written as follwing based on a predefined shape treatment \cite{combine shapes}
\begin{equation}
\label{eq:shapeparam}
\kappa_{ijk}(\alpha_j)= 1 + \frac{1}{2}((\delta^+ - \delta^-)\alpha_j + \frac{1}{8}(\delta^+ + \delta^-)(3\alpha_j^6-10\alpha_j^4+15\alpha_j^2))
\end{equation}
the $\delta^\pm$ components are ratios of the up and down shape variations, $\lambda^{up/down}$, to the nominal shape< $\lambda^{nominal}$ such that $\delta^+ = \lambda^{up}/\lambda^{nominal}$ and $\delta^- = \lambda^{down}/\lambda^{nominal}.$

From the Likelihood Equation \ref{eq:fit} which is composed of the three types of nuisances from equations \ref{eq:rateparam}, \ref{eq:logparam}, \ref{eq:shapeparam}. These nuisances are mapped to either a set of processes or shapes and also mapped to a set of associated bins. The product of the three $\kappa$ factors multiply the nominal expectation $\lambda$ such that they maximize the likelihood and in turn the agreement between the observed data $\vec{x}$ and $\vec{\lambda}$. 


\section{Definitions and Development of Modeling Systematics}
The set of nuisances that based on the types described in the previous section went through an extensive evolution beginning with very early fits with only 10 nuisances in \cite{erich thesis}. This fit only used a single nuisance to describe b-tag systematics, one for MET trigger systematics, one for luminosity and one for each background process rate. The final configuration consists of over 200 nuisances which can be divided into 5 subcategories: kinematic, process normalizations, lepton fakes, lepton categorization, and b-tagging. The optimizations of these nuisances, that is, their bin association and process mapping along with their allowed degrees of freedom has undergone extensive study. The complete list of systematics, their type, and prior uncertainties are listed in Table X.

% table of systematics
\begin{tabular}{ccc}
\hline 
Category Mapping & $N_L$ Mapping & $N_{jets}^S$  Mapping \\ 
\hline 
\hline
$p_T^{ISR}$  & 0 & $[1,\geq5]$ \\ 
$p_T^{ISR}$ & 1 & $[0,\geq4]$ \\ 
$p_T^{ISR}$ (QCD) & 0 & $[0,\geq5]$ \\ 
$\gamma_\perp$ & 0 & $[2,\geq4]$ \\ 
$\gamma_\perp$ & 1 & $[1,\geq4]$ \\ 
$\gamma_\perp$ & 2 & $[0,\geq2]$ \\  
$\gamma_\perp$ (QCD) & 0 & $[2,\geq4]$ \\ 
\hline 
\end{tabular} 


\begin{tabular}{lc}

\multicolumn{2}{|l}{Process Mapping per $(N_\ell,N_{jet}^S)$ } \\ 
\hline 
 & (Combined/All) or ($tt+jets$) or (not $tt+jets$)  \\ 
\multicolumn{2}{|l}{Category Mapping per $(N_\ell,N_{jet}^S)$ } \\ 
\hline 
 & $(N_{b-tag}^{ISR},N_{b-tag}^S)=\{(0,1),(1,0),(1,1),( \text{inclusive} ,\geq2) \}$ \\ 
\multicolumn{2}{|l}{Combined/All Nuisances}  \\ 
\hline 
 & $(N_\ell,N_{jet}^S)=\{(0,1),(1,1),(2,1),(2,2) \}$ \\ 

\end{tabular} 




\begin{tabular}{cc|ccc}

 &  & \multicolumn{3}{c}{$N_\ell$ Mapping} \\  
 &  & 1$\ell$ & 2$\ell$ & 3$\ell$ \\ 
\hline 
\multirow{6}{*}{
\rotatebox[origin=c]{90}{$N_{jet}^S$ Mapping}}  & Inclusive &  & $(ee,\mu\mu,e\mu)\times$!Gold & $(Z*,noZ*)$ \\ 

 & 0J & $(e,\mu)\times$(Gold,!Gold) & \makecell{$(OS,SS)\times$ Gold \\ $(\ell\ell)\times$(Gold,!Gold)} &   \\ 
 
 & 1J & $\ell$ Gold & $(Z*,noZ*)$ Gold &  \\ 
 
 & 2J & $\ell$ Gold & $(Z*,noZ*)$ Gold &  \\ 
 
 & 3J & $\ell$ Gold &  &  \\ 
 
 & 4J & $\ell$ Gold &  &  \\ 

\end{tabular} 


\begin{tabular}{ccc}
\hline 
Category Mapping/ Degrees of Freedom & Process Mapping & Param. Details \\ 
\hline 
\hline
per  $(N_\ell =1,2,3  \, ,N_{jet}^S)$ & W+jets & hierarchy \\ 
 
per $(N_\ell = 0 \, , N_{jet}^S)$ & (W+jets)+(ZDY) & hierarchy \\ 
 
per $(N_\ell, N_{jet}^S)$ & tt+jets & hierarchy \\ 
 
per $(N_\ell=0,1,2 \,, N_{jet}^S)$ & QCD & $0\ell$ floating otherwise $20\%$ prior \\ 
per $N_\ell=1,2,3$ & (ZDY)+(DB) & $20\%$ prior \\ 
per $N_\ell=0,1,2$ & ST & $20\%$ prior \\ 
global & TB & $20\%$ prior \\ 
global & ZDY & free floating \\ 
global & DB & free floating \\ 
\hline 
\end{tabular} 

\begin{tabular}{ccc}

Category Mapping & Process Mapping & Parameter Details \\ 
\hline 
\hline
$2_\ell^\pm\ell^\pm$ & Global & Free floating \\ 
 
Global & $(e,\mu)\times(HF,LF)$ & Free floating \\ 
 
Silver & $(e,\mu)\times(HF,LF)$ & $20\%$ prior \\ 
 
Bronze & $(e,\mu)\times(HF,LF)$ & $20\%$ prior \\ 
 
per $(N_\ell^S,N_{jet}^S)$ & $(e,\mu)\times(R_{ISR}^{shape}, M_\perp^{shape})$ & $5\%$ prior \\ 
\hline 
\end{tabular} 


\begin{tabular}{cc}
Category mapping & process mapping \\ 
\hline 
\hline
$N_{SV}^S=1$ & All \\ 
 
$N_{SV}^S \geq 1$ & All \\ 
 
$|\eta_{SV}^f|$ & tt+jets \\ 
 
$|\eta_{SV}^f|$ & other \\ 
\hline 
\end{tabular} 

\begin{tabular}{|c|c|}
\hline 
Systematic Source and Scale Factors & Parameter Type \\ 
\hline 
Luminosity & log-normal \\  
$e,\mu$ Efficiency & log-normal \\ 
b-tag efficiency & log-normal \\ 
*Factorization, Renormalization, PDF, and $Q^2$ & log-normal \\ 
*JES \& type-I MET & shape \\ 
*Unclustered Energy & shape \\ 
MET Trigger Efficiency & shape \\ 
Lepton FastSIM SF & log-normal \\ 
SV FastSIM SF & log-normal \\ 
b-jet FastSim SF & log-normal \\ 
*MET FastSIm Correction & shape \\ 
\hline 
\end{tabular} 



The optimization of each group of systematics was conducted over CR fits to 2016 data and then expanded to include multiple years then account for systematic effects induced from different run conditions. The statistical evaluation of each fit is performed by comparing metrics such as the $\log\mathcal{L}$, $\chi^2$, significance of the residuals of each bin (pull distributions), and impacts distributions which are a series of separate fits that independently vary all of the nuisance parameters to assess the impact of each on the POI. Multiple definitions of pulls are used together the main one being $ (O-E)/(E+\sigma_{postfit})$ where the $O$ is the observed data, $E$ is the expected events post-fit, the denonimator is also the expected post-fit events under the assumption that each bin is poisson distributed. The total error of the denominator is then the sum in quadrature of the poisson variance and the post fit variance for the bin in question. Similar pull defintions assist this one such as the same evaulation withough post-fit variance or the pull evaluated with respect to the pre-fit values. Each fit evaluation is performed on subsets of the same fit which includes all bins, each individual lepton multiplicity, or gold silver or bronze. The statisical errors for each bin are assumed to be gaussian with sufficient number of events. In the case of bins with very few events they assumed to be poisson distributed, this is important distinction when combining different bins, which can vary in numbers of events by orders of magnitude, into a pull distribution. For instance if a deviation of a few observed events were to occur in a bin with only a few expected events, the pull or z-score, would be significantly large which shows that the gaussian assumption in the z-score is inaccurate. To correct this assumption, each bin is given the ``Poisson treatment" new z-score is calculated from the poisson probality of the original observation given the expectation. The recipe for calculating the adjusted z-scores is as follows
\begin{itemize}
\item[1.] Generate N trials, each with new expectation $E_i' \sim \mathcal{N}(E,1)$
\item[2.] Generate new observations $O_i'\sim \text{Poisson}(E_i')$
\item[3.] Count $k$ successes such that $O',E'$ follow the original observation $(O<E \text{or} O>E)$
\item[4.] Compute Poisson probability $P=k/N$ 
\item[5.] Translate P into a z-score with normal distribution quantile
\item[6.] Compute error on z-score with up/down variations of binomial error
\item[7.] Sign z-score based on $O>E \, (+)$ or $O<E \, (-)$ convention
\end{itemize} 

The consequence of the poisson treatment is that the z-score significance is reduced in low statistics cases where $O>E$ and increased in cases with $O<E$ this is due to the asymmetery in the poisson distribution with a long tail tending to higher values.
 
%definition of sytematics
%poisson pulls

\section{Region Results}

CR and VR plots 
pull dists

